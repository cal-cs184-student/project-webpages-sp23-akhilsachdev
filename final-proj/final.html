<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <style>
            body {
              padding: 100px;
              width: 1000px;
              margin: auto;
              text-align: left;
              font-weight: 300;
              font-family: 'Open Sans', sans-serif;
              color: #121212;
            }
            h1, h2, h3, h4 {
              font-family: 'Source Sans Pro', sans-serif;
            }
          </style>
          <title>CS 184 Final Project</title>
          <meta http-equiv="content-type" content="text/html; charset=utf-8" />
          <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">
    </head>
    <body>
        <h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
        <h1 align="middle">Final Project: Multi-Shot HDR</h1>
        <h2 align="middle">Akhil Sachdev, Andy Chen, William Race, Larry Gan</h2>
        <br><br>

        <h2 align="left">Abstract</h2>
            <p>The goal of this project is to implement Multi-Shot HDR using burst photos. To do this we aligned the images, denoised the image with temporal and chroma denoising, and then we did exposure fusion to get the final output. The purpose of this was to increase the dynamic range of photos in order to generate higher quality images.</p>
        <h2 align="left">Technical Approach</h2>
        <h3 align="middle-left">Image Alignment</h3>
            <p>When taking bursts of images, we don’t have a guarantee that all of the images are necessarily aligned. Thus, before being able to do HDR and denoising, we have to have an alignment step to make sure our images are aligned. The paper authors faced constraints of having to take images on phones, so they had to make their alignment process automatic and fast. 
                Thus, as described in section 4 of the paper, they implemented alignment using <a href="https://en.wikipedia.org/wiki/Phase_correlation">phase correlation</a>, which involves computing a Fourier transform, doing a cross-power spectrum on the results, inversing the transform, and finding the argmax to determine the amount of the shift. While phase correlation is fast and efficient, it isn’t very resistant to rotation, furthermore, we had access to more powerful desktop processors and weren't constrained by processing speed. As a result, we decided to use a <a href="https://inst.eecs.berkeley.edu/~cs194-26/fa22/Lectures/feature-alignment-Part-1.pdf">feature based alignment</a> method instead of phase correlation.</p>
                <p>Our method takes in a folder of unaligned images and outputs the aligned version of all of those images. We do this by selecting the first image as our base image and finding good matches of points between that point and all other images using the following steps:</p>
                <ol type="1">
                    <li>We load a grayscale version of our image and detect all Harris corners (which are scale invariant corners) in all images; these are the initial candidates we will use to align all of them.</li>
                    <div align="middle">
                        <table style="width:100%">
                            <tr align="center">
                                <td>
                                    <img src="a/all_harris_0.png" align="middle" width="400px" />
                                </td>
                                <td>
                                    <img src="a/all_harris_1.png" align="middle" width="400px" />
                                </td>
                            </tr>
                            <br />
                            <tr align="center">
                                <td>
                                    <img src="a/all_harris_2.png" align="middle" width="400px" />
                                </td>
                                <td>
                                    <img src="a/all_harris_3.png" align="middle" width="400px" />
                                </td>
                            </tr>
                            <br />
                            <tr align="center">
                                <td>
                                    <img src="a/all_harris_4.png" align="middle" width="400px" />
                                </td>
                                <td>
                                    <img src="a/all_harris_5.png" align="middle" width="400px" />
                                </td>
                            </tr>
                            <tr align="center">
                                <td>
                                    <img src="a/all_harris_6.png" align="middle" width="400px" />
                                </td>
                            </tr>
                        </table>
                    </div>
                    <br />
                    <li>Our Harris corner detector returns a lot of possible candidates, and obviously not all of them are useful. Thus we
                        run Adaptive Non-Maximal Suppression (ANMS) on our candidate Harris corners to filter out the redundant and weak candidates. To do this, we loop through all of the points and calculate the smallest radius where that point is the strongest. We store that radius with the point and sort all of the points by radius in descending order. We then take the largest 2000 radii.
                        Here are the results, notice how the points are more spread out now.
                    </li>
                    <table style="width:100%">
                        <tr align="center">
                            <td>
                                <img src="a/features_kept_0.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_kept_1.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/features_kept_2.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_kept_3.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/features_kept_4.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_kept_5.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <tr align="center">
                            <td>
                                <img src="a/features_kept_6.png" align="middle" width="400px" />
                            </td>
                        </tr>
                    </table>
                    <li>Now that we know where the features are, we will extract the features at the location of every Harris corner in order to compare features in the future. We take a 40x40 slice of the image at each selected Harris corner’s location, normalize the slice, and resize into an 8x8 slice. Normalizing helps decrease the effect of differing levels of brightness and downsizing helps add a bit of rotation resistance, allowing us to have a more robust merge. 
                    </li>
                    <li>Once we have all of our features, we will use normalized cross correlation to compare how similar each feature in image 1 is compared to features in all the other images. If the best match between image 1 and another image is significantly better (our threshold was 5 times better) than the second best match, we would consider it a good match and keep it for the next step.</li>
                    <table style="width:100%">
                        <tr align="center">
                            <td>
                                <img src="a/features_matched_0.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_matched_1.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/features_matched_2.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_matched_3.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/features_matched_4.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/features_matched_5.png" align="middle" width="400px" />
                            </td>
                        </tr>
                    </table>
                    <li>After we have all the matching points, we use Random Sample Consensus to find a good homography matrix to transform every image into the same space as image 1. In order to do RANSAC, we select four feature pairs at random, compute the homography matrix between those 4 pairs, find the inliers where the distance between the transformed and original point are below 2, and keep the largest set of inliers. We repeat the process 1000 times and use the largest set of inliers to compute our final homography</li>
                    <table style="width:100%">
                        <tr align="center">
                            <td>
                                <img src="a/ransac_kept_1.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/ransac_kept_2.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/ransac_kept_3.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/ransac_kept_4.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/ransac_kept_5.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/ransac_kept_6.png" align="middle" width="400px" />
                            </td>
                        </tr>
                    </table>
                    <li>Once we have a final homography for each image, we load a colored version of the image, pad its edges to avoid losing bits of the image during the transform, and use skimage’s transform function to transform that image into the space of image 1.</li>
                    <table style="width:100%">
                        <tr align="center">
                            <td>
                                <img src="a/img_warped_0.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/img_warped_1.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/img_warped_2.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/img_warped_3.png" align="middle" width="400px" />
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/img_warped_4.png" align="middle" width="400px" />
                            </td>
                            <td>
                                <img src="a/img_warped_5.png" align="middle" width="400px" />
                            </td>
                        </tr>
                    </table>
                    <li>Lastly we crop all the images to only the area where they all align and save those cropped images in order to do temporal denoising on them. We also create a separate image by averaging the cropped images together in an attempt to reduce noise.</li>
                    <table style="width:100%">
                        <tr align="center">
                            <td>
                                <img src="a/final_intermediate_0.png" align="middle" width="400px" />
                                <figcaption>Final image still padded + corners for crop</figcaption>
                            </td>
                            <td>
                                <img src="a/final_cropped_orig_5.png" align="middle" width="400px" />
                                <figcaption>Final image no pad</figcaption>
                            </td>
                        </tr>
                        <br />
                        <tr align="center">
                            <td>
                                <img src="a/all_warped_final.png" align="middle" width="400px" />
                                <figcaption>Final cropped</figcaption>
                            </td>
                        </tr>
                        <br />
                    </table>
                </ol>
                <p>Optimizing the code to work for large images was a pretty large challenge. Our initial iteration of the alignment code relied only on python for loops and was super slow. It took around 10 minutes to create aligned pairs of small, 600x800 pixel images. Since a lot of the runtime scales in polynomial time, this unoptimized approach was not going to work for the larger 3000x4000 camera pictures we were planning on aligning. As a result, we poured significant efforts into increasing the speed of our code. The most successful approach was to replace as many for loops with numpy vectorization as possible. This was able to decrease runtimes for all functions significantly. We also tried was to add numba’s @jit decorators to all of our functions. Numba’s @jit decorator tries to compile python code into fast machine code, and we tried it first because it seemed like an easy way to decrease runtime free. Unfortunately, we weren’t able to get the decorator to work for all the functions due to type errors, so we just used numpy vectorization. Another big optimization we did was to switch our feature matching code from manually comparing each feature using for loops to doing feature comparisons using a matrix multiplication.<p>
                
                <p>Here are some examples of the speedups we got, times are in seconds:</p>
                <ul>
                    <li>Old ANMS: 271.2730002403259 (This time came from 2 runs of the function)</li>
                    <li>New ANMS: 0.5030028820037842</li>
                    <li>Old Feature Matching: 21.665003061294556 (This time came from 2 runs of the function)</li>
                    <li>New Feature Matching jit: 7.206876516342163</li>
                    <li>New Feature Matching matrix: 0.0739743709564209</li>
                    <li>Old RANSAC: 30.82899832725525 (This time came from 2 runs of the function)</li>
                    <li>New RANSAC: 5.108975172042847</li>
                    <li>Feature Extraction was actually pretty fast so no changes were needed</li>
                </ul>

                <p>I definitely learned a lot more about numpy vectorization and array slicing throughout this experience. I wasn’t completely comfortable using numpy before, but I’m definitely more familiar with it now. I also learned a bit about jit and how it can be used to speed things up, even if I wasn’t able to make things work in the end. Lastly, it will always be surprising to me how well just randomly selecting points to compute homographies works.</p>
        <h3 align="left">Temporal Denoising</h3>

            <p>We implemented temporal denoising, described in section 5 of the paper, Merging Frames. This involved splitting the image into separate 16x16 tiles and computing the 2D Fourier transforms of each tile to convert them to the frequency domain. Then we average across all Fourier transformed images for each tile before computing the inverse 2D Fourier transform on the averaged result to retrieve the transformed image data. After we’ve converted all the averages tiles back from the fourier transform, we stitch the entire transformed image back together. This process is repeated for every color channel and the channels were recombined into a rgb image at the end. Instead of just computing the average of the transformed images, the paper uses a more complicated formula that is meant to account for alignment failure. However, since we used a different alignment algorithm that we believe to be more robust, we opted to use the naive formula because it would save on the computation time of calculating the noise variance and other array calculations.
            </p>
            <p>When implementing this part of the project, we had trouble getting the actual image to appear, and many of our outputs were randomly colored pixels. This was because we were not converting to and from the frequency domain properly, resulting in a strange output. We fixed this by using the 2-D inverse FFT instead of the 1-D iFFT. Once we fixed this problem, the output image resembled the original image, but there were still strange artifacts in the result. After some debugging, we found that the issue was converting the arrays with np.real. When aggregating the values of the array to calculate the average, we ran into an issue with trying to add complex64 types to float64 types. We decided to turn the complex64 array to real numbers because we thought the imaginary part of the number was insignificant. However, we found that when using np.add and keeping the numbers complex, we could reconstruct the original image without the artifacts.</p>
            <p>One thing we learned from implementing this part of the code was the importance of writing code that was easy to debug. While Andy coded the initial implementation of the code, it was difficult to debug and read because some parts of the algorithm were not implemented efficiently and seemed confusing. Larry was able to improve upon the code by using techniques like numpy array slicing and condensed the code by using fewer variables and having more straightforward steps, making it easier to understand what was happening in each line of the code. This made it easier to isolate the bugs in the code and fix them.</p>
        <h3 align="left">Chroma Denoising</h3>
            <p>Before exposure fusion we use chroma denoising to remove red and green patches from darker areas in low-light images. To do this we convert the image from BGR to YUV to isolate the chrominance channels (U and V), which are paired to the Red and Green of the image. We then do bilateral filtering twice on each the U and the V channel. We don’t modify the Y channel because the Y represents the luminance channel and the goal of chroma denoising is to minimize patches of color pixels. The reason we use bilateral filtering is so that we’re able to preserve the important details of the image while selectively blurring out the appropriate pixels. After bilateral filtering, we merge the filtered U and V channels to the original Y channel, which is then converted to a BGR image once again.</p>
            <p>There isn’t a stark difference between the original image and chromatically denoised image because it’s a very minute finishing touch that’s used to improve the coloring of images. One of the challenges was the conceptual understanding of the changing of color channels and the purpose of each channel.</p>

        <h3 align="left">Exposure Fusion</h3>
            <p>After denoising, we use tone mapping to produce HDR images. In particular, we use the method of synthetic exposures – we apply gain and gamma correction to produce several copies of the image at varied exposure levels. Then we merge these exposures into a final image according to weights. The Hasinoff et al. paper we referenced uses a simple weighting system which ranks pixels by the Euclidean distance of the pixel intensity to well-exposedness (0.5) however we use the Gaussian kernel instead of Euclidean distance so that the results are smooth. We also boost contrast by taking the Laplacian of the exposure and preferring pixels that have a high gradient. This will sometimes result in bright outlines around high contrasting objects in the scene so to fix this we only use this weight to boost dark pixels. We do this by multiplying the gradients weight by the inverse intensity of the pixel. This gets rid of the bright outlines while still creating the high contrast look we are looking for. </p>
            <p>Avoiding a cartoony result is a recurring difficulty in the exposure fusion process. The reference paper uses only two synthetic exposures however this severely limits the ability of the program to improve scenes with a high dynamic range without resulting in a cartoony image. We find that using about 4-6 exposures provides the best results. This allows us to have even differences in gain between the exposures which provides smoother transitions when the images are stitched together. In choosing gain values, we assume that the original image is on average underexposed to begin with so we create one darker exposure and several brighter exposures. While our process is slower than the reference paper’s implementation, it produces a better result. Our approach is therefore a middle ground between phone HDR and exposure bracketing. Since we use synthetic exposures, the main advantage of our approach is that we do not have to vary exposure when taking a burst of images to feed into our pipeline. This means that capturing a scene is still quick for the photographer and we avoid motion blur from having to take longer exposures. </p>
        
        <iframe align="center" width="500" height="500" src="https://www.youtube.com/embed/IwJ5Wko4QJ4">
        </iframe>
        <p>Disclaimer: The aligned image was automatically generated</p>

        <a href="https://docs.google.com/presentation/d/1xO0b1G6vld3S39woMyLxuwqrshqqG0cyYltDW2p24a4/edit?usp=sharing">Link to Slides</a>
    </body>
</html>